@misc{Alibaba,
  title         = {From Known to Unknown: Knowledge-guided Transformer for Time-Series Sales Forecasting in Alibaba},
  author        = {Xinyuan Qi and Kai Hou and Tong Liu and Zhongzhong Yu and Sihao Hu and Wenwu Ou},
  year          = {2021},
  eprint        = {2109.08381},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2109.08381}
}

@article{ARIMA,
  author  = {Stellwagen, E. and Tashman, Len},
  year    = {2013},
  month   = {01},
  pages   = {28-33},
  title   = {ARIMA: The Models of Box and Jenkins},
  journal = {Foresight: Int. J. Appl. Forecast.}
}

@inproceedings{Attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title     = {Attention is all you need},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6000–6010},
  numpages  = {11},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{Autoformer,
  author    = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  title     = {Autoformer: decomposition transformers with auto-correlation for long-term series forecasting},
  year      = {2021},
  isbn      = {9781713845393},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38\% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.},
  booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
  articleno = {1717},
  numpages  = {12},
  series    = {NIPS '21}
}

@misc{CNNTimeseries,
  title         = {Conditional Time Series Forecasting with Convolutional Neural Networks},
  author        = {Anastasia Borovykh and Sander Bohte and Cornelis W. Oosterlee},
  year          = {2018},
  eprint        = {1703.04691},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1703.04691}
}

@inproceedings{crossformer,
  title     = {Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting},
  author    = {Yunhao Zhang and Junchi Yan},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year      = {2023},
  url       = {https://openreview.net/forum?id=vSVLM2j9eie}
}

@misc{decoder_only,
  title         = {A decoder-only foundation model for time-series forecasting},
  author        = {Abhimanyu Das and Weihao Kong and Rajat Sen and Yichen Zhou},
  year          = {2024},
  eprint        = {2310.10688},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.10688}
}

@misc{Hanshuo2024,
  title        = {2024实体零售未来白皮书},
  author       = {汉硕},
  year         = {2024},
  howpublished = {\url{https://max.book118.com/html/2024/0402/8072140065006053.shtm}}
}

@misc{hurun2023,
  title        = {中国新零售白皮书({China New Retail Report 2023})},
  author       = {胡润百富},
  year         = {2023},
  howpublished = {\url{https://res.hurun.cn/file/20231218/20231218150314249.pdf}}
}

@article{informer,
  author  = {Haoyi Zhou and
             Jianxin Li and
             Shanghang Zhang and
             Shuai Zhang and
             Mengyi Yan and
             Hui Xiong},
  title   = {Expanding the prediction capacity in long sequence time-series forecasting},
  journal = {Artificial Intelligence},
  volume  = {318},
  pages   = {103886},
  issn    = {0004-3702},
  year    = {2023}
}

@article{itransformer,
  title   = {iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  author  = {Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  journal = {arXiv preprint arXiv:2310.06625},
  year    = {2023}
}

@inproceedings{kitaev2020reformer,
  title     = {Reformer: The Efficient Transformer},
  author    = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgNKkHtvB}
}

@inproceedings{LightGBM,
  author    = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  title     = {LightGBM: a highly efficient gradient boosting decision tree},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {3149–3157},
  numpages  = {9},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{liu2022pyraformer,
  title     = {Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting},
  author    = {Shizhan Liu and Hang Yu and Cong Liao and Jianguo Li and Weiyao Lin and Alex X. Liu and Schahram Dustdar},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=0EXmFzUn5I}
}

@article{ltsm,
  title   = {Understanding Different Design Choices in Training Large Time Series Models},
  author  = {Chuang*, Yu-Neng and Li*, Songchen and Yuan*, Jiayi and Wang*, Guanchu and Lai*, Kwei-Herng and Yu, Leisheng and Ding, Sirui and Chang, Chia-Yuan and Tan, Qiaoyu and Zha, Daochen and Hu, Xia},
  journal = {arXiv preprint arXiv:2406.14045},
  year    = {2024}
}

@article{M5_3rd,
  title     = {Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series},
  volume    = {38},
  issn      = {0169-2070},
  url       = {http://dx.doi.org/10.1016/j.ijforecast.2022.01.001},
  doi       = {10.1016/j.ijforecast.2022.01.001},
  number    = {4},
  journal   = {International Journal of Forecasting},
  publisher = {Elsevier BV},
  author    = {Nasios, Ioannis and Vogklis, Konstantinos},
  year      = {2022},
  month     = oct,
  pages     = {1448–1459}
}

@article{MethodExpanding,
  author  = {Haoyi Zhou and
             Jianxin Li and
             Shanghang Zhang and
             Shuai Zhang and
             Mengyi Yan and
             Hui Xiong},
  title   = {Expanding the prediction capacity in long sequence time-series forecasting},
  journal = {Artificial Intelligence},
  volume  = {318},
  pages   = {103886},
  issn    = {0004-3702},
  year    = {2023}
}

@book{nndlbook,
  title     = {神经网络与深度学习},
  publisher = {机械工业出版社},
  year      = {2020},
  author    = {邱锡鹏},
  address   = {北京},
  isbn      = {9787111649687},
  url       = {https://nndl.github.io/}
}

@inproceedings{onefitsall,
  title     = {One Fits All: Power General Time Series Analysis by Pretrained LM},
  author    = {Tian Zhou{,} Peisong Niu{,} Xue Wang{,} Liang Sun{,} Rong Jin},
  booktitle = {NeurIPS},
  year      = {2023}
}

@inproceedings{PatchTST,
  title     = {A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author    = {Nie, Yuqi and
               H. Nguyen, Nam and
               Sinthong, Phanwadee and 
               Kalagnanam, Jayant},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}

@inproceedings{Random_Forests,
  author    = {Tin Kam Ho},
  booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
  title     = {Random decision forests},
  year      = {1995},
  volume    = {1},
  number    = {},
  pages     = {278-282 vol.1},
  keywords  = {Classification tree analysis;Decision trees;Training data;Optimization methods;Testing;Tin;Stochastic processes;Handwriting recognition;Hidden Markov models;Multilayer perceptrons},
  doi       = {10.1109/ICDAR.1995.598994}
}

@article{RNNTimeseries,
  title    = {Recurrent Neural Networks for Time Series Forecasting: Current status and future directions},
  journal  = {International Journal of Forecasting},
  volume   = {37},
  number   = {1},
  pages    = {388-427},
  year     = {2021},
  issn     = {0169-2070},
  doi      = {https://doi.org/10.1016/j.ijforecast.2020.06.008},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207020300996},
  author   = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
  keywords = {Big data, Forecasting, Best practices, Framework},
  abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.}
}

@article{SVM,
  author   = {Cortes, Corinna
              and Vapnik, Vladimir},
  title    = {Support-vector networks},
  journal  = {Machine Learning},
  year     = {1995},
  month    = {Sep},
  day      = {01},
  volume   = {20},
  number   = {3},
  pages    = {273-297},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  issn     = {1573-0565},
  doi      = {10.1007/BF00994018},
  url      = {https://doi.org/10.1007/BF00994018}
}

@book{TimeSeriesAnalysis,
  title     = {时间序列分析与应用},
  publisher = {机械工业出版社},
  year      = {2011},
  author    = {JonathanD. Cryer},
  isbn      = {9787111325727}
}

@inproceedings{TimesNet,
  title     = {TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author    = {Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}

@inproceedings{TimeXer,
  author    = {Wang, Yuxuan and Wu, Haixu and Dong, Jiaxiang and Qin, Guo and Zhang, Haoran and Liu, Yong and Qiu, Yunzhong and Wang, Jianmin and Long, Mingsheng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages     = {469--498},
  publisher = {Curran Associates, Inc.},
  title     = {TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0113ef4642264adc2e6924a3cbbdf532-Paper-Conference.pdf},
  volume    = {37},
  year      = {2024}
}

@inproceedings{Transformer-LSTM,
  author    = {Liu, Yining and Wang, Ziyao},
  booktitle = {2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)},
  title     = {Enhanced Transformer-LSTM Daily Sales Forecasting Model Based on Attention Mechanism},
  year      = {2024},
  volume    = {},
  number    = {},
  pages     = {730-734},
  keywords  = {Deep learning;Measurement;Attention mechanisms;Accuracy;Limiting;Predictive models;Transformers;Forecasting;Robots;Long short term memory;Sales Forecasting;Deep Learning;Transformer;Attention Mechanisms;Long Short-Term Memory},
  doi       = {10.1109/RICAI64321.2024.10911533}
}

@article{Transformer综述,
  author    = {孟祥福{,} 石皓源},
  title     = {基于Transformer模型的时序数据预测方法综述},
  publisher = {计算机科学与探索},
  year      = {2025},
  journal   = {计算机科学与探索},
  volume    = {19},
  number    = {1},
  eid       = {45},
  pages     = {45-64},
  keywords  = {深度学习；时序数据预测；数据预处理；Transformer模型},
  doi       = http://fcst.ceaj.org/CN/10.3778/j.issn.1673-9418.2403070
}
@inproceedings{XGBoost,
  title      = {XGBoost: A Scalable Tree Boosting System},
  url        = {http://dx.doi.org/10.1145/2939672.2939785},
  doi        = {10.1145/2939672.2939785},
  booktitle  = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  publisher  = {ACM},
  author     = {Chen, Tianqi and Guestrin, Carlos},
  year       = {2016},
  month      = aug,
  pages      = {785–794},
  collection = {KDD ’16}
}