\begin{thebibliography}{10}

\bibitem{CNNTimeseries}
Anastasia Borovykh, Sander Bohte, and Cornelis~W. Oosterlee.
\newblock Conditional time series forecasting with convolutional neural networks, 2018.

\bibitem{XGBoost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, page 785–794. ACM, August 2016.

\bibitem{ltsm}
Yu-Neng Chuang*, Songchen Li*, Jiayi Yuan*, Guanchu Wang*, Kwei-Herng Lai*, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, and Xia Hu.
\newblock Understanding different design choices in training large time series models.
\newblock {\em arXiv preprint arXiv:2406.14045}, 2024.

\bibitem{SVM}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine Learning}, 20(3):273--297, Sep 1995.

\bibitem{TimeSeriesAnalysis}
JonathanD. Cryer.
\newblock {\em 时间序列分析与应用}.
\newblock 机械工业出版社, 2011.

\bibitem{decoder_only}
Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou.
\newblock A decoder-only foundation model for time-series forecasting, 2024.

\bibitem{RNNTimeseries}
Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara.
\newblock Recurrent neural networks for time series forecasting: Current status and future directions.
\newblock {\em International Journal of Forecasting}, 37(1):388--427, 2021.

\bibitem{Random_Forests}
Tin~Kam Ho.
\newblock Random decision forests.
\newblock In {\em Proceedings of 3rd International Conference on Document Analysis and Recognition}, volume~1, pages 278--282 vol.1, 1995.

\bibitem{onefitsall}
Tian Zhou{,} Peisong Niu{,} Xue Wang{,} Liang Sun{,}~Rong Jin.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock In {\em NeurIPS}, 2023.

\bibitem{LightGBM}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
\newblock Lightgbm: a highly efficient gradient boosting decision tree.
\newblock In {\em Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, page 3149–3157, Red Hook, NY, USA, 2017. Curran Associates Inc.

\bibitem{kitaev2020reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu2022pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X. Liu, and Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Transformer-LSTM}
Yining Liu and Ziyao Wang.
\newblock Enhanced transformer-lstm daily sales forecasting model based on attention mechanism.
\newblock In {\em 2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)}, pages 730--734, 2024.

\bibitem{itransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock {\em arXiv preprint arXiv:2310.06625}, 2023.

\bibitem{M5_3rd}
Ioannis Nasios and Konstantinos Vogklis.
\newblock Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series.
\newblock {\em International Journal of Forecasting}, 38(4):1448–1459, October 2022.

\bibitem{PatchTST}
Yuqi Nie, Nam H.~Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{Alibaba}
Xinyuan Qi, Kai Hou, Tong Liu, Zhongzhong Yu, Sihao Hu, and Wenwu Ou.
\newblock From known to unknown: Knowledge-guided transformer for time-series sales forecasting in alibaba, 2021.

\bibitem{ARIMA}
E.~Stellwagen and Len Tashman.
\newblock Arima: The models of box and jenkins.
\newblock {\em Foresight: Int. J. Appl. Forecast.}, pages 28--33, 01 2013.

\bibitem{Attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, \L{}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.

\bibitem{TimeXer}
Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang, Yong Liu, Yunzhong Qiu, Jianmin Wang, and Mingsheng Long.
\newblock Timexer: Empowering transformers for time series forecasting with exogenous variables.
\newblock In A.~Globerson, L.~Mackey, D.~Belgrave, A.~Fan, U.~Paquet, J.~Tomczak, and C.~Zhang, editors, {\em Advances in Neural Information Processing Systems}, volume~37, pages 469--498. Curran Associates, Inc., 2024.

\bibitem{TimesNet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{Autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: decomposition transformers with auto-correlation for long-term series forecasting.
\newblock In {\em Proceedings of the 35th International Conference on Neural Information Processing Systems}, NIPS '21, Red Hook, NY, USA, 2021. Curran Associates Inc.

\bibitem{crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{informer}
Haoyi Zhou, Jianxin Li, Shanghang Zhang, Shuai Zhang, Mengyi Yan, and Hui Xiong.
\newblock Expanding the prediction capacity in long sequence time-series forecasting.
\newblock {\em Artificial Intelligence}, 318:103886, 2023.

\bibitem{MethodExpanding}
Haoyi Zhou, Jianxin Li, Shanghang Zhang, Shuai Zhang, Mengyi Yan, and Hui Xiong.
\newblock Expanding the prediction capacity in long sequence time-series forecasting.
\newblock {\em Artificial Intelligence}, 318:103886, 2023.

\bibitem{Hanshuo2024}
汉硕.
\newblock 2024实体零售未来白皮书.
\newblock \url{https://max.book118.com/html/2024/0402/8072140065006053.shtm}, 2024.

\bibitem{Transformer综述}
孟祥福{,} 石皓源.
\newblock 基于transformer模型的时序数据预测方法综述.
\newblock {\em 计算机科学与探索}, 19(1):45--64, 2025.

\bibitem{hurun2023}
胡润百富.
\newblock 中国新零售白皮书({China New Retail Report 2023}).
\newblock \url{https://res.hurun.cn/file/20231218/20231218150314249.pdf}, 2023.

\end{thebibliography}
